{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing & Evaluation for OpenAI SDK Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Tracing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's load our environment variables from our .env file. Make sure all of the keys necessary in .env.example are included!\n",
    "\n",
    "In your `.env` file, set up the following fields \n",
    "\n",
    "LANGSMITH_TRACING=\"true\" \\\n",
    "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\" \\\n",
    "LANGSMITH_API_KEY=\"your-api-key\" \\\n",
    "LANGSMITH_PROJECT=\"your-project-name\" \\\n",
    "OPENAI_API_KEY=\"your-openai-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading agent and setting up sample instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the `MultiAgentQAEvaluator` from qa_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/qiaocatherine/git/snorkel-ai-evaluation\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from qa_agent.evaluators import MultiAgentQAEvaluator, MultiAgentQAEvaluatorParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will trace the application using OpenAI Agents SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip -q install \"langsmith[openai-agents]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, set_trace_processors\n",
    "from langsmith.wrappers import OpenAIAgentsTracingProcessor\n",
    "\n",
    "# Set up tracing processor\n",
    "set_trace_processors([OpenAIAgentsTracingProcessor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke agent and trace! \n",
    "\n",
    "Example [trace](https://smith.langchain.com/public/6e2bb491-8357-4799-abb8-2619b2cfddbf/r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Passed: True\n",
      "Notes: The provided answer is correct with confidence 0.95. The rationale follows: The provided answer correctly identifies the cat as a tuxedo cat based on the visual characteristics observed in the image. The image shows a cat with a predominantly black fur pattern and distinct white markings on the chest, paws, and face, which are characteristic of a tuxedo cat. This matches the definition of a tuxedo cat as a bicolor cat with a specific black-and-white pattern. The validation tasks consistently support this conclusion with high confidence, and there is no contradictory evidence. Therefore, the answer is correct..\n",
      "Confidence: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Configure the evaluator\n",
    "parameters = MultiAgentQAEvaluatorParameters(\n",
    "    planner_model=\"gpt-4o-mini\",\n",
    "    executor_model=\"gpt-4o\",\n",
    "    agent_timeout=90,\n",
    "    low_confidence_threshold=0.5\n",
    ")\n",
    "\n",
    "# Create evaluator instance\n",
    "evaluator = MultiAgentQAEvaluator(parameters)\n",
    "\n",
    "# Example: Cat image validation\n",
    "question = \"What type of cat is shown in this image?\"\n",
    "provided_answer = \"This is a tuxedo cat with black and white fur pattern.\"\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/George%2C_a_perfect_example_of_a_tuxedo_cat.jpg/1250px-George%2C_a_perfect_example_of_a_tuxedo_cat.jpg\"\n",
    "\n",
    "# Run validation\n",
    "result = await evaluator.evaluate(\n",
    "    question=question,\n",
    "    provided_answer=provided_answer,\n",
    "    image_url=image_url\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"Validation Passed: {result.passed}\")\n",
    "print(f\"Notes: {result.notes}\")\n",
    "\n",
    "if result.metadata.get(\"confidence\"):\n",
    "    print(f\"Confidence: {result.metadata['confidence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Complex Evaluations (full agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import StaticPool\n",
    "\n",
    "def get_engine_for_chinook_db():\n",
    "    \"\"\"Pull sql file, populate in-memory database, and create engine.\"\"\"\n",
    "    url = \"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\"\n",
    "    response = requests.get(url)\n",
    "    sql_script = response.text\n",
    "\n",
    "    connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n",
    "    connection.executescript(sql_script)\n",
    "    return create_engine(\n",
    "        \"sqlite://\",\n",
    "        creator=lambda: connection,\n",
    "        poolclass=StaticPool,\n",
    "        connect_args={\"check_same_thread\": False},\n",
    "    )\n",
    "\n",
    "engine = get_engine_for_chinook_db()\n",
    "db = SQLDatabase(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Evaluations** are made up of three components:\n",
    "\n",
    "1. A **dataset test** inputs and expected outputs.\n",
    "2. An **application or target function** that defines what you are evaluating, taking in inputs and returning the application output\n",
    "3. **Evaluators** that score your target function's outputs.\n",
    "\n",
    "![Evaluation](../images/evals-conceptual.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads test cases from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>image_url</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Which variant is most similar to HPR1 in terms...</td>\n",
       "      <td>HPR1-T335A</td>\n",
       "      <td>https://d28jlkw5i7tp0z.cloudfront.net/upload/p...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Which solid at 10 WT% has the highest deviatio...</td>\n",
       "      <td>CRB</td>\n",
       "      <td>https://d28jlkw5i7tp0z.cloudfront.net/upload/p...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Examining the gene expression patterns across ...</td>\n",
       "      <td>CYP1A2</td>\n",
       "      <td>https://d28jlkw5i7tp0z.cloudfront.net/upload/p...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which treatment most increases both AP-1 and C...</td>\n",
       "      <td>H2O2</td>\n",
       "      <td>https://d28jlkw5i7tp0z.cloudfront.net/upload/p...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>After application of depolarizing current, at ...</td>\n",
       "      <td>10' + 30'</td>\n",
       "      <td>https://d28jlkw5i7tp0z.cloudfront.net/upload/p...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           question      answer  \\\n",
       "0      0  Which variant is most similar to HPR1 in terms...  HPR1-T335A   \n",
       "1      1  Which solid at 10 WT% has the highest deviatio...         CRB   \n",
       "2      2  Examining the gene expression patterns across ...      CYP1A2   \n",
       "3      3  Which treatment most increases both AP-1 and C...        H2O2   \n",
       "4      4  After application of depolarizing current, at ...   10' + 30'   \n",
       "\n",
       "                                           image_url  is_correct  \n",
       "0  https://d28jlkw5i7tp0z.cloudfront.net/upload/p...        True  \n",
       "1  https://d28jlkw5i7tp0z.cloudfront.net/upload/p...        True  \n",
       "2  https://d28jlkw5i7tp0z.cloudfront.net/upload/p...        True  \n",
       "3  https://d28jlkw5i7tp0z.cloudfront.net/upload/p...        True  \n",
       "4  https://d28jlkw5i7tp0z.cloudfront.net/upload/p...        True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"agent_qa_eval_samples.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset in LangSmith, inputs to evaluators will include `question`, `answer`, and `image_url`; reference_output will be `is_correct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"Snorkel AI: Complex Evaluation\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        client.create_examples(\n",
    "            inputs=[{\"question\": row[\"question\"], \"answer\": row[\"answer\"],\"image_url\": row[\"image_url\"]}],\n",
    "            outputs=[{\"is_correct\": row[\"is_correct\"]}],\n",
    "            dataset_id=dataset.id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define Application Logic to be Evaluated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define how to run our agent, taking in the input fields as defined in our dataset. \n",
    "\n",
    "**You can easily configure the paramters to run a different experiment, allowing you to test between different models & configurations' impact on the agent quality**. For other components like agent parameters, you can easily change them within the agent, import, and run the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the evaluator\n",
    "parameters = MultiAgentQAEvaluatorParameters(\n",
    "    planner_model=\"gpt-4o-mini\",\n",
    "    executor_model=\"gpt-4o-mini\",\n",
    "    agent_timeout=100, \n",
    "    low_confidence_threshold=0.5\n",
    ")\n",
    "\n",
    "# Create evaluator instance\n",
    "evaluator = MultiAgentQAEvaluator(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent(inputs: dict):\n",
    "    \"\"\"Run agent and track the final response.\"\"\"\n",
    "\n",
    "    result = await evaluator.evaluate(\n",
    "        question=inputs[\"question\"],\n",
    "        provided_answer=inputs[\"answer\"],\n",
    "        image_url=inputs[\"image_url\"]\n",
    "    )\n",
    "\n",
    "    return {\"result\": result.passed, \"notes\": result.notes, \"confidence\": result.metadata['confidence']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using heuristic code evaluator**\n",
    "\n",
    "Since our human reference output and the actual result are in boolean, we can do a simple heuristic evaluator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def alignment(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the agent chose the correct route.\"\"\"\n",
    "    return outputs['result'] == reference_outputs[\"is_correct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run the Evaluation\n",
    "\n",
    "Example result [here](https://smith.langchain.com/public/a08a7fcf-4094-4888-b645-82755d9e0623/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation job and results\n",
    "experiment_results = await client.aevaluate(\n",
    "    run_agent,\n",
    "    data=dataset_name,\n",
    "    evaluators=[alignment],\n",
    "    experiment_prefix=\"agent-gpt-4o-mini\",\n",
    "    num_repetitions=1, # since LLMs are non-deterministic, you can run multiple iterations over the same example in a dataset\n",
    "    max_concurrency=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
